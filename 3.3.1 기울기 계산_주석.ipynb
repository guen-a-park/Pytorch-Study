{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.3.1 기울기 계산 (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YVRrHBx9e6rr"},"source":["# 단순한 기울기 계산 \n","\n","- z = 2x^2+3\n","\n"]},{"cell_type":"code","metadata":{"id":"Cy8X-XnTe7sI"},"source":["# 먼저 파이토치를 불러옵니다.\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"84i6uWfmPAF-"},"source":["# x를 [2.0,3.0]의 값을 가진 텐서로 초기화 해주고 기울기 계산을 True로 켜 놓습니다. \n","# z = 2x^2+3\n","\n","# x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n","# y = x**2\n","# z = 2*y +3\n","\n","x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n","y = x**2\n","z = 2*y + 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMvOPaVGet2q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611395049811,"user_tz":-540,"elapsed":808,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}},"outputId":"e966d53b-7ad0-4342-9fed-7fd95a43dd8a"},"source":["# https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.autograd.backward\n","\n","# 목표값을 지정합니다. \n","target = torch.tensor([3.0,4.0])\n","\n","# z와 목표값의 절대값 차이를 계산합니다. \n","# backward는 스칼라 값에 대해서 동작하기 때문에 길이 2짜리 텐서인 loss를 torch.sum을 통해 하나의 숫자로 바꿔줍니다.\n","loss = torch.sum(torch.abs(z-target))\n","\n","# 그리고 스칼라 값이 된 loss에 대해 backward를 적용합니다.\n","loss.backward()\n","\n","# 여기서 y와 z는 기울기가 None으로 나오는데 이는 x,y,z중에 x만이 leaf node이기 때문입니다.\n","\n","print(x.grad, y.grad, z.grad) # 8->z미분 후 2대입,12->z미분 후 3대입 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 8., 12.]) None None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TcMR-USL7rvy"},"source":[""],"execution_count":null,"outputs":[]}]}