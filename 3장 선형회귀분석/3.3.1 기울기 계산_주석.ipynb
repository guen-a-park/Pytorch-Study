{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.3.1 기울기 계산_주석.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YVRrHBx9e6rr"},"source":["# 단순한 기울기 계산 \n","\n","- z = 2x^2+3\n","\n"]},{"cell_type":"code","metadata":{"id":"Cy8X-XnTe7sI","executionInfo":{"status":"ok","timestamp":1615386222404,"user_tz":-540,"elapsed":3412,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}}},"source":["# 먼저 파이토치를 불러옵니다.\n","import torch"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"84i6uWfmPAF-","executionInfo":{"status":"ok","timestamp":1615386225427,"user_tz":-540,"elapsed":636,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}}},"source":["# x를 [2.0,3.0]의 값을 가진 텐서로 초기화 해주고 기울기 계산을 True로 켜 놓습니다. \n","# z = 2x^2+3\n","\n","# x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n","# y = x**2\n","# z = 2*y +3\n","\n","x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n","y = x**2\n","z = 2*y + 3"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMvOPaVGet2q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615386229281,"user_tz":-540,"elapsed":1856,"user":{"displayName":"­박근아(엘텍공과대학 휴먼기계바이오공학부)","photoUrl":"","userId":"13921555510394140081"}},"outputId":"e2dfd5af-bda4-43e6-d0d4-ed9233174615"},"source":["# https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.autograd.backward\n","\n","# 목표값을 지정합니다. \n","target = torch.tensor([3.0,4.0])\n","\n","# z와 목표값의 절대값 차이를 계산합니다. \n","# backward는 스칼라 값에 대해서 동작하기 때문에 길이 2짜리 텐서인 loss를 torch.sum을 통해 하나의 숫자로 바꿔줍니다.\n","loss = torch.sum(torch.abs(z-target))\n","\n","# 그리고 스칼라 값이 된 loss에 대해 backward를 적용합니다.\n","loss.backward()\n","\n","# 여기서 y와 z는 기울기가 None으로 나오는데 이는 x,y,z중에 x만이 leaf node이기 때문입니다.\n","\n","print(x.grad, y.grad, z.grad) # 8->z미분 후 2대입,12->z미분 후 3대입 "],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([ 8., 12.]) None None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TcMR-USL7rvy"},"source":[""],"execution_count":null,"outputs":[]}]}
